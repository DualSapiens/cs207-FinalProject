{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation with *autodiff*\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The *autodiff* package implements [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation), a technique for computing derivatives of functions that is distinct from both symbolic and numerical differentiation. \n",
    "\n",
    "The merit of automatic differentiation is its ability to handle complicated functions without sacrificing the accuracy of the computed derivative. Whereas symbolic differentiation guarantees accuracy, it can be intractable for complex functions. By contrast, numerical differentiation affords ease in implementation and is amenable to any function, but it comes at the expense of accuracy. Automatic, or algorithmic, differentation is an alternative approach which allows derivatives to be computed by a computer program up to machine precision.\n",
    "\n",
    "Automatic differentiation in science and engineering has been applied to address problems in optimization, root-finding, and implicit time-integration.\n",
    "\n",
    "## Background\n",
    "\n",
    "The underlying principle behind automatic differentiation is the chain rule. Any complex function whose derivative we would like to compute can be expressed as the composition of elementary functions and operations. By obeying properties of the chain rule in tracing how the elementary operations make up the final function, an accompanying derivative of the function can be computed.\n",
    "\n",
    "There are two flavors of algorithm for implementing automatic differentiation: the forward mode and the reverse mode. Here we will focus on the forward mode, which means that derivatives are computed starting from the most primitive building block - the independent variables of the function - to the most complex: the function itself. Following is a simple example of how the forward mode may be used determine the derivative of\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    f(x,y) = \\log \\left[ 1 + \\exp \\left( x^2 - y^2 \\right) \\right]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the process by constructing a computational graph. The first two nodes of our graph, $x_1$ and $x_2$, take as input the independent variables of the function $f$. Their gradients $\\left(\\dfrac{\\partial x_i}{\\partial x},\\dfrac{\\partial x_i}{\\partial y}\\right)$ have the seed values $(1,0)$ and $(0,1)$, respectively.\n",
    "![](figs/cg1.png)\n",
    "Next, $x_3$ and $x_4$ are exponentiated to obtain $x_1$ and $x_2$, and their corresponding gradients are defined in terms of $x_1$ and $x_2$, computed in the previous step.\n",
    "![](figs/cg2.png)\n",
    "$x_5$ is composed by a subtraction operation between $x_3$ and $x_4$, and followed by raising $e$ to $x_5$ to obtain $x_6$.\n",
    "![](figs/cg3.png)\n",
    "Following through the remaining operations, we arrive at $x_8$, which corresponds to the final function $f$:\n",
    "![](figs/cg4.png)\n",
    "\n",
    "This procedure also extends naturally to vector-valued functions, for which the computational graph would end with multiple output nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software Organization\n",
    "\n",
    "The following is a proposed directory structure and relevant modules we plan to implement in our package:\n",
    "\n",
    "```\n",
    "README.md\n",
    "docs\\\n",
    "    milestone1.ipynb\n",
    "    examples\\\n",
    "        intro_AD.py\n",
    "        multivariate_AD.py\n",
    "        vector_valued_AD.py\n",
    "        special_functions.py\n",
    "autodiff\\\n",
    "    autodiff\\\n",
    "        __init__.py\n",
    "        autodiff.py\n",
    "        math.py\n",
    "        tests\\\n",
    "            __init__.py\n",
    "            test_autodiff.py\n",
    "    setup.py\n",
    "    LICENSE\n",
    "    README.md\n",
    "    .travis.yml\n",
    "    setup.cfg\n",
    "```\n",
    "\n",
    "The key modules in this package are:\n",
    "1. **autodiff\\autodiff.py**: defines the core structure of automatic differentiation, including   attributes for accessing values and gradients, and methods for operator overloading. This is the main module users will import.\n",
    "\n",
    "2. **autodiff\\math.py**: defines elementary functions (sin, exp) to be imported by the user and used when defining a function.\n",
    "\n",
    "4. **docs\\examples\\intro_AD.py**: An introductory example of using the AutoDiff package, covering univariate, scalar valued functions.\n",
    "5. **examples\\multivariate_AD.py**: A demonstration of automatic differentiation for multivariate functions, as well as retrieving the attributes of vector-valued functions.\n",
    "6. **examples\\special_functions.py**: How to import and use functions such as `sin` and `exp` when defining functions.\n",
    "7. **autodiff\\tests\\test_autodiff.py**: A module of test functions that we will use in validating our implementation.\n",
    "\n",
    "The test suite will be in the tests directory of our package. We envision maintaining our test suite and package on GitHub, with Travis CI and Coveralls for continuous integration. We plan to look into PyPI for our package distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use the *autodiff* package\n",
    "\n",
    "The following three examples illustrate some common uses of the `autodiff` package.\n",
    "#### Univariate, scalar-valued function\n",
    "After importing `autodiff`, a `Var` object is created which represents an independent variable. A function f defined in terms of this object will be differentiated, and its derivative with respect to x accessed through the method `der(x)`.\n",
    "\n",
    "```Python\n",
    "from autodiff import autodiff\n",
    "\n",
    "x = autodiff.Var()\n",
    "\n",
    "f = 5*x**2\n",
    "\n",
    "x.set_value(5)\n",
    "print(f.der(x)) # prints the derivative of f with respect to x\n",
    "```\n",
    "\n",
    "#### Multivariate, scalar-valued function\n",
    "A multivariable function is defined by initializing more than one `Var` object. Then, the derivative with respect to each independent variable may be accessed by passing in the appropriate variable in the argument of the `der()` method. The `grad()` method allows several derivatives to be returned at once. Here we also illustrate the use of special functions `sin` and `cos`, imported from the `autodiff` math module.\n",
    "\n",
    "```Python\n",
    "from autodiff import autodiff\n",
    "from autodiff import math as admath\n",
    "\n",
    "x = autodiff.Var()\n",
    "y = autodiff.Var()\n",
    "\n",
    "f = admath.sin(x)**2 + 2*admath.cos(y)\n",
    "\n",
    "x.set_value(5)\n",
    "y.set_value(3)\n",
    "print(f.der(x)) # prints the derivative of f with respect to x\n",
    "print(f.der(y)) # prints the derivative of f with respect to y\n",
    "assert f.grad([x,y]) == [f.der(x), f.der(y)]\n",
    "```\n",
    "\n",
    "#### Multivariate, vector-valued function\n",
    "Vector-valued functions can be defined using the `Array` class of `autodiff`. Then, the `der.()` method returns a list of derivatives whose length is equal to the number of components of f, while the `grad()` method can return a list of lists - the complete Jacobian of the function. \n",
    "\n",
    "```Python\n",
    "from autodiff import autodiff\n",
    "\n",
    "x = autodiff.Var()\n",
    "y = autodiff.Var()\n",
    "\n",
    "f = autodiff.Array([5*x**2+3*y,\n",
    "                    3*x+2*y**2])\n",
    "\n",
    "x.set_value(5)\n",
    "y.set_value(3)\n",
    "assert f.der(x) == [f[0].der(x),\n",
    "                    f[1].der(x)]\n",
    "                    \n",
    "assert f.grad([x,y]) == [[f[0].der(x), f[0].der(y)],\n",
    "                         [f[1].der(x), f[1].der(y)]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Core data structures include:\n",
    "\n",
    "- A list for unique IDs that identify operations\n",
    "\n",
    "#### The `Operation` Class\n",
    "\n",
    "The `Operation` Class forms the core data structure for automatic differentiation. Subclasses of the `Operation` include `Var`, `Constant`, operators, and elementary functions. Its attributes and methods are the following:\n",
    "\n",
    "```\n",
    "Class Operation:\n",
    "      Attributes: \n",
    "          ID\n",
    "          _value\n",
    "      Methods:\n",
    "          __eq__(self, other)\n",
    "          __add__(self, other)\n",
    "          __radd__(self, other)\n",
    "          __sub__ (self, other)\n",
    "          __rsub__ (self, other)\n",
    "          __mul__(self, other)\n",
    "          __rmul__(self, other)\n",
    "          __div__ (self, other)\n",
    "          __rdiv__ (self, other)\n",
    "          __neg__ (self)\n",
    "          __pow__(self, power, modulo=None)\n",
    "          __rpow__(self, other)\n",
    "          evaluate(self)\n",
    "          der(self, op)\n",
    "```\n",
    "    \n",
    "#### The `Var` Subclass\n",
    "Instances of the `Var` subclass represent independent variables. User-defined functions that can be differentiated by our package are expressed in terms of `Var` instances. The `Var` Class inherits from its `Operation` superclass, and defines an additional method:\n",
    "\n",
    "```\n",
    "Class Var(Operation):\n",
    "      Methods:\n",
    "          set_value(self, value)\n",
    "```\n",
    "\n",
    "#### The `Constant` Subclass\n",
    "The `Constant` Subclass allows our package to handle constants in a user-defined function.\n",
    "\n",
    "#### Elementary operations\n",
    "Elementary operations include `Addition`, `Multiplication`, `Subtraction`, `Division`, `Negation`, `Power`, and overload the existing operations. They are defined as subclasses of `Operation`. User-defined functions involving these operations will be instances of an operation subclass, corresponding to the operation applied at the last step in the computational graph.\n",
    "\n",
    "#### Elementary functions\n",
    "Elementary functions include `sin`, `cos`, `tan`, and their inverses,`exp` and `log`, and `abs`. A user wishing to define a function with these operations must import them from the `autodiff.math` module prior to defining the function. As with elementary operations, user-defined functions using elementary functions will be instances of the elementary function if it is the last step applied in the computational graph. An external dependency of our package is `numpy`, which allows us to carry out the elementary operations within our subclasses.\n",
    "\n",
    "#### The `Array` Class\n",
    "The `Array` Class is used to define vector-valued functions. A user wishing to define a vector valued function can express their function as an instance of `Array`, which takes as its argument a list of the components of the function. Its methods are the following:\n",
    "\n",
    "```\n",
    "Array:\n",
    "      Methods:\n",
    "          der(self, op)\n",
    "```\n",
    "\n",
    "#### The `IDAllocator` Class\n",
    "The `IDAllocator` Class is a helper class that allocates a unique ID to operations and variables. Its attributes and methods are the following:\n",
    "\n",
    "```\n",
    "IDAllocator:\n",
    "      Attributes:\n",
    "          ids\n",
    "      Methods:\n",
    "          allocate_id(cls)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Python3]",
   "language": "python",
   "name": "Python [Python3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
